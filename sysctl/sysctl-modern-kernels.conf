# SYSCTL For Modern 4.9+ kernels
# Default sysctl example tuned file that can work as a base for tuning, it should work for majority of cases where virtual machines have at least 8GB of RAM and more
# All parameters are actively being documented with references, so you can also calculate more realistic numbers if needed, however in most cases it shouldn't really cause any side effects
# Some of the commented parameters have a few notes, depending on the deployment and scenario to make sure issues do not arise, so please, do go through all of these with caution
# Most of the uncommented parameters should be safe for most cases but as mentioned before, you can tune them closer to your needs if necessary.
# Most of these will work without problems on deployments that have Kernels higher than 4.9+. A few of the parameters are documented with this information to make it easier to pick the best option.

# Increase TCP/UDP pages (min, pressure, max) 
net.ipv4.tcp_mem = 94500000 915000000 927000000
net.ipv4.udp_mem = 10240 87380 33554432

# The read-buffer for TCP/UDP (min, default, max) in bytes
net.ipv4.tcp_rmem = 32768 33554432 134217728
net.ipv4.udp_rmem_min = 16384

# The default setting of the socket receive buffer in bytes
net.core.rmem_default = 33554432

# The maximum receive socket buffer size in bytes
net.core.rmem_max = 268435456

# TCP write-buffer (min, default, max) in bytes
net.ipv4.tcp_wmem = 32768 33554432 134217728

# UDP write-buffer (min) in bytes
net.ipv4.udp_wmem_min=16384

# The default setting (in bytes) of the socket send buffer
net.core.wmem_default = 33554432

# The maximum send socket buffer size in bytes
net.core.wmem_max = 134217728

# Controls path MTU discovery (Change to 1 if jumbo frames are enabled, jumbo frames ARE NOT supported on Azure)
# 0 = Disabled, 1 = Disabled by default, enabled if an ICMP blackhole is detected, 2 = Always enabled, use initial value of tcp_base_mss.
net.ipv4.tcp_mtu_probing = 0

# By default, TCP saves various connection metrics in the route cache when the connection closes, so that connections established in the near future can use these to set initial conditions.
# Usually, this increases overall performance, but it might sometimes cause performance degradation.
# If set, TCP will not cache metrics on closing connections.
net.ipv4.tcp_no_metrics_save = 0

# If set, TCP performs receive buffer auto-tuning, attempting to automatically size the buffer (no greater than tcp_rmem) to match the size required by the path for full throughput.
# Enabled by default.
net.ipv4.tcp_moderate_rcvbuf = 1

# Turn on TCP window scaling
net.ipv4.tcp_window_scaling = 1

# Limit the maximum memory used to reassemble IP fragments (CVE-2018-5391)
net.ipv4.ipfrag_low_thresh = 196608
net.ipv6.ip6frag_low_thresh = 196608
net.ipv4.ipfrag_high_thresh = 262144
net.ipv6.ip6frag_high_thresh = 262144

# Prevent SYN attack, enable SYNcookies (they will kick-in when the max_syn_backlog reached)
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_syn_retries = 2
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_max_syn_backlog = 65535

# Decrease the default tcp_fin_timeout connection time
net.ipv4.tcp_fin_timeout = 10

# Decrease the time default value for connections to keep alive
net.ipv4.tcp_keepalive_time = 300
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_intvl = 15

# Turn on the tcp_timestamps, accurate timestamp make TCP congestion control algorithms work better
# NOTE: For deployments where the Linux VM is BEHIND an Azure Load Balancer, timestamps MUST be set to 0
net.ipv4.tcp_timestamps = 1

# Try to reuse TCP connections / Usually recommended to be enabled. If problems are seen, the recommendation is to set it to 2 (default) to reuse only loopback connections.
# For SAP deployments, sometimes they will require tcp_timestamps to be enabled to work with TCP reuse, so if tcp_timestamps are disabled because of load balancers, you should set reuse to 2.
net.ipv4.tcp_tw_reuse = 1

# Allowed local port range. This will increase the number of locally available ports (source ports)
net.ipv4.ip_local_port_range = 1024 65535

# Enable a fix for RFC1337 - time-wait assassination hazards in TCP
net.ipv4.tcp_rfc1337 = 1

# Use BBR TCP congestion control and set tcp_notsent_lowat to 4294967295 to ensure HTTP/2 prioritization works optimally
# Do a 'modprobe tcp_bbr' first (kernel > 4.9)
# Fall-back to htcp if bbr is unavailable (< 4.9 kernels)
# Usually recommended to use BBR for long range connections (WAN/Internet) / HTCP for close communication (same VNET)
# IF kernel < 4.9 use:
# net.ipv4.tcp_congestion_control = htcp
# IF kernel > 4.9 use: 
net.ipv4.tcp_congestion_control = bbr

# A TCP socket can control the amount of unsent bytes in its write queue, thanks to TCP_NOTSENT_LOWAT socket option. 
# poll()/select()/epoll() reports POLLOUT events if the amount of unsent bytes is below a per socket value, and if the write queue is not full. sendmsg() will also not add new buffers if the limit is hit.
# This global variable controls the amount of unsent data for sockets not using TCP_NOTSENT_LOWAT. 
# For these sockets, a change to the global variable has immediate effect.
net.ipv4.tcp_notsent_lowat = 4294967295

# For servers with tcp-heavy workloads, enable 'fq' queue management scheduler (kernel > 3.12)
net.core.default_qdisc = fq

# Increase the number of incoming connections / number of connections backlog
net.core.somaxconn = 32768
net.core.netdev_max_backlog = 32768
net.core.dev_weight = 64

# Increase the maximum amount of option memory buffers
net.core.optmem_max = 65535

# Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks
net.ipv4.tcp_max_tw_buckets = 360000

# Limit number of orphans, each orphan can eat up to 16M (max wmem) of unswappable memory
net.ipv4.tcp_max_orphans = 16384
net.ipv4.tcp_orphan_retries = 0

# Don't allow the arp table to become bigger than this
net.ipv4.neigh.default.gc_thresh3 = 2048

# Tell the gc when to become aggressive with arp table cleaning.
# Adjust this based on size of the LAN. 1024 is suitable for most /24 networks
net.ipv4.neigh.default.gc_thresh2 = 1024

# Adjust where the gc will leave arp table alone - set to 32.
net.ipv4.neigh.default.gc_thresh1 = 64

# Adjust to arp table gc to clean-up more often
net.ipv4.neigh.default.gc_interval = 30

# Increase TCP queue length
net.ipv4.neigh.default.proxy_qlen = 96
net.ipv4.neigh.default.unres_qlen = 6

# Enable Explicit Congestion Notification (RFC 3168), disable it if it doesn't work for you
net.ipv4.tcp_ecn = 1
net.ipv4.tcp_reordering = 3

# How many times to retry killing an alive TCP connection
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_retries1 = 3

# For SAP Hana environments, set the tcp_syn_retries to 8 which will round up to about 190 seconds
net.ipv4.tcp_syn_retries = 8
net.ipv4.tcp_synack_retries = 2

# Avoid falling back to slow start after a connection goes idle
# keeps our cwnd large with the keep alive connections (kernel > 3.6)
net.ipv4.tcp_slow_start_after_idle = 0

# If listening service is too slow to accept new connections, reset them. Default state is FALSE. It means that if overflow #occurred due to a burst, connection will recover. Enable this option _only_ if you are really sure that listening daemon cannot #be tuned to accept connections faster. Enabling this option can harm clients of your server.
net.ipv4.tcp_abort_on_overflow = 1

# Maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are # registered to polling are probed in a round-robin manner.
net.core.netdev_budget = 600

# Allow the TCP fastopen flag to be used, beware some firewalls do not like TFO! (kernel > 3.7)
net.ipv4.tcp_fastopen = 3

# Optimize input packet processing down to one demux for certain kinds of local sockets. Currently we only do this for established TCP and connected UDP sockets
net.ipv4.ip_early_demux = 0
net.ipv4.tcp_early_demux = 0
net.ipv4.udp_early_demux = 0

# Disable reverse path forwarding
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.eth0.rp_filter = 0
net.ipv4.conf.eth1.rp_filter = 0
net.ipv4.conf.lo.rp_filter = 0
net.ipv4.tcp_frto = 0

# Applicable to SUSE when tuning pagecache and dirty ratios in large VMs (1TB or more of RAM)
#vm.pagecache_limit_mb = 20972                  # 20gb - Different values could be tried from say 20gb <> 64gb
#vm.pagecache_limit_ignore_dirty = 1            # see the below section on this variable to decide what it should be set too

# Recommended to set on VMs with large amounts of memory (96GB+), but it can also be set on smaller VMs as well. It will usually require some testing to understand if this is helping or not.
# References:
# https://www.suse.com/support/kb/doc/?id=000017857
# https://www.suse.com/support/kb/doc/?id=000019008
vm.dirty_bytes = 629145600

# Set this value to approximately 50% of vm.dirty_bytes
vm.dirty_background_bytes = 314572800

# Disable the ratio since these are not recommended on VMs with large amounts of RAM - They should automatically be disabled once # the bytes are set but, best practices to set them to 0
vm.dirty_background_ratio = 0
vm.dirty_ratio = 0
