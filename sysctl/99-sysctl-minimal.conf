## Default Sysctl Tuned File: Example for Virtual Machines with â‰¥8GB RAM
#This example sysctl configuration serves as a base for system tuning. It is designed to work in most scenarios involving virtual machines with at least **8GB of RAM** or more.

### Key Details:
#- **Parameter Documentation**: Each parameter is actively documented with references, enabling you to calculate more precise values tailored to your specific environment. For most setups, the default values should function without causing adverse side effects.
#- **Commented Parameters**: Some parameters are commented out and include notes specific to certain deployments or scenarios. Carefully review these notes to avoid potential issues.
#- **Uncommented Parameters**: The parameters left uncommented are considered safe defaults for most environments. However, you can further customize them based on your workload and requirements.
#- **Kernel Compatibility**: This configuration is optimized for **Linux Kernels version 4.9 or higher**. Where necessary, parameters have been documented with kernel-specific details to simplify selection.
#- **Modern Defaults**: For kernels newer than 4.9, modern parameters are uncommented by default to ensure optimal performance and compatibility.

### Recommendations:
#Please review all settings thoroughly before applying them to your environment. While this tuned file should work well in most cases, specific adjustments may be required to match your exact requirements and avoid unforeseen issues.

# Increase TCP/UDP pages (min, pressure, max) 
net.ipv4.tcp_mem = 4096 87380 67108864
net.ipv4.udp_mem = 4096 87380 33554432

# This is set MAX to 64MB
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864

# The default setting of the socket receive buffer in bytes
# This is set to 32MB
net.core.rmem_default = 33554432
net.core.wmem_default = 33554432

# UDP write-buffer (min) in bytes
# UDP read-buffer (min) in bytes
net.ipv4.udp_wmem_min = 16384
net.ipv4.udp_rmem_min = 16384

# The maximum send socket buffer size in bytes
# The value of 128MB should work in majority of cases in Azure, if you do have VM's capable of doing 100G+ you can increase it to the max allowed which is 2GB (2147483647)
net.core.wmem_max = 134217728
net.core.rmem_max = 134217728

# Low latency busy poll timeout for poll and select. (needs CONFIG_NET_RX_BUSY_POLL) Approximate time in us to busy loop waiting for events. 
# Recommended value depends on the number of sockets you poll on. For several sockets 50, for several hundreds 100. 
# For more than that you probably want to use epoll. 
# Note that only sockets with SO_BUSY_POLL set will be busy polled, so you want to either selectively set SO_BUSY_POLL on those sockets or set sysctl.net.busy_read globally. 
# Will increase power usage. Default: 0 (off)
net.core.busy_poll = 50
net.core.busy_read = 50

# Controls path MTU discovery (Change to 1 if jumbo frames are enabled, jumbo frames ARE NOT supported on Azure)
# 0 = Disabled, 1 = Disabled by default, enabled if an ICMP blackhole is detected, 2 = Always enabled, use initial value of tcp_base_mss.
net.ipv4.tcp_mtu_probing = 0

# By default, TCP saves various connection metrics in the route cache when the connection closes, so that connections established in the near future can use these to set initial conditions.
# Usually, this increases overall performance, but it might sometimes cause performance degradation.
# If set, TCP will not cache metrics on closing connections.
net.ipv4.tcp_no_metrics_save = 0

# If set, TCP performs receive buffer auto-tuning, attempting to automatically size the buffer (no greater than tcp_rmem) to match the size required by the path for full throughput.
# Enabled by default.
net.ipv4.tcp_moderate_rcvbuf = 1

# Turn on TCP window scaling
net.ipv4.tcp_window_scaling = 1

# Prevent SYN attack, enable SYNcookies (they will kick-in when the max_syn_backlog reached)
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_syn_retries = 2
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_max_syn_backlog = 65535

# Turn on the tcp_timestamps, accurate timestamp make TCP congestion control algorithms work better
# NOTE: For deployments where the Linux VM is BEHIND an Azure Load Balancer, timestamps MUST be set to 0
net.ipv4.tcp_timestamps = 1

# Try to reuse TCP connections / Usually recommended to be enabled. If problems are seen, the recommendation is to set it to 2 (default) to reuse only loopback connections.
# For SAP deployments, sometimes they will require tcp_timestamps to be enabled to work with TCP reuse, so if tcp_timestamps are disabled because of load balancers, you should set reuse to 2.
# This parameter will greatly improve outgoing connections, it will not impact incoming connections.
net.ipv4.tcp_tw_reuse = 1

# Allowed local port range. This will increase the number of locally available ports (source ports)
net.ipv4.ip_local_port_range = 1024 65535

# Enable a fix for RFC1337 - time-wait assassination hazards in TCP
net.ipv4.tcp_rfc1337 = 1

# Use BBR TCP congestion control and set tcp_notsent_lowat to 4294967295 to ensure HTTP/2 prioritization works optimally
# Do a 'modprobe tcp_bbr' first (kernel > 4.9)
# Fall-back to htcp if bbr is unavailable (< 4.9 kernels)
# Usually recommended to use BBR for long range connections (WAN/Internet) / HTCP for close communication (same VNET)
# IF kernel < 4.9 use:
# net.ipv4.tcp_congestion_control = htcp
# IF kernel > 4.9 use: 
net.ipv4.tcp_congestion_control = bbr

# For servers with tcp-heavy workloads, enable 'fq' queue management scheduler (kernel > 3.12)
net.core.default_qdisc = fq

# Increase the number of incoming connections / number of connections backlog
net.core.somaxconn = 32768
net.core.netdev_max_backlog = 32768
net.core.dev_weight = 64

# Increase the maximum amount of option memory buffers
# For high-performance environments, it's recommended to increase from the default 20KB to 65KB, in some extreme cases, for environments that support 100G+ networking, you can 
# increase it to 1048576
net.core.optmem_max = 65535

# Enable Explicit Congestion Notification (RFC 3168), disable it if it doesn't work for you
net.ipv4.tcp_ecn = 1
net.ipv4.tcp_reordering = 3

# How many times to retry killing an alive TCP connection
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_retries1 = 3

# For SAP Hana environments, set the tcp_syn_retries to 8 which will round up to about 190 seconds
net.ipv4.tcp_syn_retries = 8
net.ipv4.tcp_synack_retries = 2

# The net.ipv4.tcp_slow_start_after_idle setting determines whether TCP connections should re-enter the slow start phase after being idle for a certain period.
# Keeps our cwnd large with the keep alive connections (kernel > 3.6)
# Disable (0) for High-Throughput Applications: In environments like Apache Cassandra deployments or other high-throughput systems, disabling this setting can improve performance by maintaining the CWND even after idle periods.
# Enable (1) for General Use: For most general-purpose workloads, keeping this setting enabled ensures fairness and stability, especially in shared network environments.
net.ipv4.tcp_slow_start_after_idle = 0

# If listening service is too slow to accept new connections, reset them. Default state is FALSE. It means that if overflow #occurred due to a burst, connection will recover. Enable this option _only_ if you are really sure that listening daemon cannot #be tuned to accept connections faster. Enabling this option can harm clients of your server.
net.ipv4.tcp_abort_on_overflow = 1

# Maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are # registered to polling are probed in a round-robin manner.
net.core.netdev_budget = 1000

# Allow the TCP fastopen flag to be used, beware some firewalls do not like TFO! (kernel > 3.7)
# The net.ipv4.tcp_fastopen setting enables TCP Fast Open (TFO), a feature designed to reduce latency during the TCP handshake by allowing data to be sent in the initial SYN packet
net.ipv4.tcp_fastopen = 3

# Reverse path forwarding
# The `net.ipv4.conf.default.rp_filter` setting controls reverse path filtering, which is a security feature designed to prevent IP spoofing by ensuring that incoming packets have a valid source address. Whether to disable it in cloud environments depends on your specific use case and network configuration.

### **When to Disable It**
#- **Asymmetric Routing**: If your cloud environment involves asymmetric routing (where incoming and outgoing packets take different paths), strict reverse path filtering can cause legitimate packets to be dropped. In such cases, disabling or relaxing this setting (e.g., setting it to `0` or `2`) might be necessary.
#- **Complex Multi-NIC Setups**: In scenarios where multiple network interfaces are used, and traffic is routed across different subnets, reverse path filtering can interfere with normal operations.

### **When to Keep It Enabled**
#- **Simple Network Topologies**: For straightforward setups where traffic flows symmetrically, keeping reverse path filtering enabled (e.g., setting it to `1`) enhances security by blocking spoofed packets.
#- **Security-Sensitive Applications**: If your application requires strict validation of incoming packets, enabling this feature can help mitigate certain types of attacks.

### **Recommendation**
#In cloud environments, it's common to set `net.ipv4.conf.default.rp_filter` to `2` (loose mode) rather than disabling it entirely. Loose mode allows packets to pass as long as the source address is reachable via any interface, which is more forgiving in complex routing scenarios while still providing some level of security.
net.ipv4.conf.all.rp_filter = 2
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.lo.rp_filter = 2

# Forward Retransmission Timeout Recovery
#F-RTO helps TCP distinguish between packet loss caused by random events (e.g., network interference) and congestion. After a retransmission timeout, it sends a probe packet to determine if the loss was random. 
#If the probe is acknowledged, TCP avoids unnecessary congestion control measures, improving performance.
net.ipv4.tcp_frto = 0

# Recommended to set on VMs with large amounts of memory (96GB+), but it can also be set on smaller VMs as well. It will usually require some testing to understand if this is helping or not.
# References:
# https://www.suse.com/support/kb/doc/?id=000017857
# https://www.suse.com/support/kb/doc/?id=000019008
vm.dirty_bytes = 629145600

# Set this value to approximately 50% of vm.dirty_bytes
vm.dirty_background_bytes = 314572800

# Disable the ratio since these are not recommended on VMs with large amounts of RAM - They should automatically be disabled once # the bytes are set but, best practices to set them to 0
vm.dirty_background_ratio = 0
vm.dirty_ratio = 0

# Decrease the swap utilization
# On 5.8+ kernels the value goes from 0-200 and the default is usually 60, setting at 0 is not recommended.
# Low Swappiness (0-20): Best suited for performance-critical environments, such as database servers (e.g., Apache Cassandra) or workloads requiring minimal disk I/O latency. This setting ensures RAM is prioritized and swap space is used sparingly, making it a great fit for most cloud environments.
# Moderate Swappiness (20-100): Suitable for general-purpose servers, allowing some swapping when necessary to balance memory usage. This range can work well for applications with mixed workloads where some disk swapping won't heavily impact performance.
# High Swappiness (100-200): Only recommended in rare cases where swap space is intentionally leveraged as an extension of RAM, such as systems with constrained physical memory and less demand on real-time performance. Be cautious, as aggressive swapping can lead to degraded performance for high-throughput applications.
vm.swappiness = 10
